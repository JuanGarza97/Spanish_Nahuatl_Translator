{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AVANCE 3. BASELINE**"
      ],
      "metadata": {
        "id": "3SWMOauDj0bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nombre del Trabajo:\n",
        "AI for Good: Human Heritage\n",
        "Automatic pre-Hispanic to Spanish Recogniser and Translator\n",
        "\n",
        "\n",
        "Nombre del asesor del proyecto: Dr. Juan Arturo Nolazco Flores\n",
        "\n",
        "Equipo: 20\n",
        "Nombre de alumnos:\n",
        " * Juan Carlos Garza Sanchez\tA00821522\n",
        " * Sinaí Avalos Rivera \t\t\tA01730466"
      ],
      "metadata": {
        "id": "h4n4LM9ljlSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FEW-SHOTS LEARNING**"
      ],
      "metadata": {
        "id": "W5g4X0U4pv5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código hace todo el proceso de tomar una frase en Nahuatl, usar un ejemplo de traducción para generar una traducción automática a Español utilizando el modelo Gemini-Pro de Google y t5 large de hugging face. Luego evalúa esa traducción utilizando las métricas METEOR, BLEU y ROUGE. Escogimos estos modelos debido a que son de uso gratuito y son ampliamente utilizados para few shots learning.\n",
        "\n",
        "Consideramos utilizar otros modelos, como mBART de Facebook por ejemplo, pero el problema de ellos es que necesitan más datos, toman más tiempo y además no estariamos haciendo few shots learning. Ya que el objetivo del proyecto es hacer traducción con few shots learning, se descartaron los modelos que no permiten few shots learning.\n",
        "\n",
        "También descartamos el uso de OpenAI por el momento debido a que no es de uso gratuito."
      ],
      "metadata": {
        "id": "SZ3uL6XUjalb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134eDCoHeD8_",
        "outputId": "71b4049c-ad56-4320-d962-546bb668715b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.11/dist-packages (1.79.0)\n",
            "Collecting nltk==3.8.1\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk==3.8.1) (4.67.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (4.25.6)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (24.2)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (1.14.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.0.7)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.14.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.27.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2025.1.31)\n",
            "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=8024f8825899751f495fcd8502b40be09a42b0ec3ea28406d1ed2a4fc3598806\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: nltk, rouge-score\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.8.1 rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-aiplatform nltk==3.8.1 rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "\n",
        "# Recursos de NLTK para tokenizacion y WordNet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "import pandas as pd\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdkzVAjbeHaZ",
        "outputId": "3dd57899-58a5-47fd-f674-90cccf0c3e50"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar la API de Gemini\n",
        "GOOGLE_API_KEY = #Insert API KEY\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "-qOPHxO8eH2q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar dataset\n",
        "df = pd.read_csv(\"hf://datasets/somosnlp-hackathon-2022/Axolotl-Spanish-Nahuatl/train.csv\")\n",
        "df = df[['sp', 'nah']].rename(columns={'sp': 'spanish', 'nah': 'nahuatl'})\n",
        "df = df.dropna(axis=0, how='any')\n",
        "df = df.astype(str)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "3fWzoGALeJCf",
        "outputId": "99025cd5-c86d-4655-fde9-25295a70739f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 spanish  \\\n",
              "0      Y así, cuando hizo su ofrenda de fuego, se sie...   \n",
              "1      ¿Si es jade, si es oro, acaso no tendrá que ir...   \n",
              "2      Y cuando el Sol estuvo solo en el cielo, enseg...   \n",
              "3      Allá acudieron asimismo los señores del cabild...   \n",
              "4                                                  Usos:   \n",
              "...                                                  ...   \n",
              "20023   El Espíritu y la esposa dicen: \"¡Ven!\"El que ...   \n",
              "20024   Yo advierto a todo el que oye las palabras de...   \n",
              "20025   y si alguno quita de las palabras del libro d...   \n",
              "20026   El que da testimonio de estas cosas dice: \"¡S...   \n",
              "20027     La gracia de nuestro Señor Jesús sea con todos   \n",
              "\n",
              "                                                 nahuatl  \n",
              "0      Auh in ye yuhqui in on tlenamacac niman ye ic ...  \n",
              "1      ¿In chalchihuitl, teocuitlatl, mach ah ca on yaz?  \n",
              "2      Auh yn oyuh in yoca hualmotlalli tonatiuh ylhu...  \n",
              "3      Yn oncan mohuicatza yhuan yn ciudad cabildo tl...  \n",
              "4                                               Kualtia:  \n",
              "...                                                  ...  \n",
              "20023  On Espíritu Santo niman isihuau on Borreguito ...  \n",
              "20024  Nemechtlachicahuilia nenmochimej yejhuan nenqu...  \n",
              "20025  Niman tla yacaj quipopolohuilis itemachtil Dio...  \n",
              "20026  On yejhuan quipantlantia yejhua in, ijquin qui...  \n",
              "20027  Ma toTeco Jesucristo mechtiochihua nenmochimej...  \n",
              "\n",
              "[20024 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-43b64511-315b-4cb9-914c-6cf55c3360b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spanish</th>\n",
              "      <th>nahuatl</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Y así, cuando hizo su ofrenda de fuego, se sie...</td>\n",
              "      <td>Auh in ye yuhqui in on tlenamacac niman ye ic ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>¿Si es jade, si es oro, acaso no tendrá que ir...</td>\n",
              "      <td>¿In chalchihuitl, teocuitlatl, mach ah ca on yaz?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Y cuando el Sol estuvo solo en el cielo, enseg...</td>\n",
              "      <td>Auh yn oyuh in yoca hualmotlalli tonatiuh ylhu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Allá acudieron asimismo los señores del cabild...</td>\n",
              "      <td>Yn oncan mohuicatza yhuan yn ciudad cabildo tl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Usos:</td>\n",
              "      <td>Kualtia:</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20023</th>\n",
              "      <td>El Espíritu y la esposa dicen: \"¡Ven!\"El que ...</td>\n",
              "      <td>On Espíritu Santo niman isihuau on Borreguito ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20024</th>\n",
              "      <td>Yo advierto a todo el que oye las palabras de...</td>\n",
              "      <td>Nemechtlachicahuilia nenmochimej yejhuan nenqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20025</th>\n",
              "      <td>y si alguno quita de las palabras del libro d...</td>\n",
              "      <td>Niman tla yacaj quipopolohuilis itemachtil Dio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20026</th>\n",
              "      <td>El que da testimonio de estas cosas dice: \"¡S...</td>\n",
              "      <td>On yejhuan quipantlantia yejhua in, ijquin qui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20027</th>\n",
              "      <td>La gracia de nuestro Señor Jesús sea con todos</td>\n",
              "      <td>Ma toTeco Jesucristo mechtiochihua nenmochimej...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20024 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43b64511-315b-4cb9-914c-6cf55c3360b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-43b64511-315b-4cb9-914c-6cf55c3360b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-43b64511-315b-4cb9-914c-6cf55c3360b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-87a74c73-2a1f-43ea-ae9b-dc4f8f8464e7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-87a74c73-2a1f-43ea-ae9b-dc4f8f8464e7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-87a74c73-2a1f-43ea-ae9b-dc4f8f8464e7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b2ca7a14-9b16-4b46-a9f0-d9b19c9431c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b2ca7a14-9b16-4b46-a9f0-d9b19c9431c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20024,\n  \"fields\": [\n    {\n      \"column\": \"spanish\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18877,\n        \"samples\": [\n          \"\\u00bfQuieres a este muchacho?\",\n          \" Ahora bien, hermanos, vosotros sois hijos de la promesa tal como Isaac\",\n          \" Por la fe Abraham, cuando fue probado, ofreci\\u00f3 a Isaac.El que hab\\u00eda recibido las promesas ofrec\\u00eda a su hijo \\u00fanico\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nahuatl\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 18942,\n        \"samples\": [\n          \"Tiyazque ye ichan\",\n          \"Auh inic onicpolo in tlacpac oncan no ninotlapololti auh nican nictlalia in huel neltiliztli in Maria Xoco yllamatzin in ipillo ytoca catca Anan Tlaco huecapa ypilo auh niman no hualla Miguel Ocelotl oquimonamicti Anan Tlaco auh niman oncan otlacat Maria Anan auh niman no quimonamictico in Pedro Luys chane Quauhchinanco zan ce in iconeuh oquichtli ytoca Estevan omomiquili auh cepa oquimonamictin yn itoca Magdalena auh niman no momiquili in Pedro Luys Quauhchinanco yc nictlalia nofirma Diego Leonardo escribano.\",\n          \"Amotlen mahuiztic.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una frase de prueba para \"One-Shot\"\n",
        "n = 20020\n",
        "nth_row = df.iloc[[n]]\n",
        "print(nth_row['nahuatl'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o34JFy3beMt9",
        "outputId": "e83d4124-d00e-480b-f58b-69163843aa74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemechtlachicahuilia nenmochimej yejhuan nenquicaquij itemachtil Dios yejhuan ipan in tlajcuilolamatl nesticaj: tla yacaj quimiyequilis in tlajtlamach, Dios quimiyequilis on plagas yejhuan ye tlajcuiloloncaj ipan in tlajcuilolamatl. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar un solo ejemplo de entrenamiento (one-shot)\n",
        "one_shot_example = df.iloc[[100]]\n"
      ],
      "metadata": {
        "id": "MaJkJQVnePPp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la función **create_one_shot_prompt** para generar un promt que instruye al modelo a traducir la frase correspondiente en Nahuatl a Español. Se da un ejemplo de traducción en el prompt y luego se solicita al modelo que traduzca la frase objetivo."
      ],
      "metadata": {
        "id": "OtZH-erHiGRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el prompt con un solo ejemplo\n",
        "def create_one_shot_prompt(target_sentence, example):\n",
        "    return f\"\"\"\n",
        "    Translate the following Nahuatl sentence to Spanish:\n",
        "    Example:\n",
        "    \"{example['nahuatl'].values[0]}\" -> \"{example['spanish'].values[0]}\"\n",
        "\n",
        "    Now, translate this sentence:\n",
        "    \"{target_sentence}\"\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "T2XwGzP3eRCZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = create_one_shot_prompt(nth_row['nahuatl'].values[0], one_shot_example)"
      ],
      "metadata": {
        "id": "i9IxSY_KeSLa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_one_shot_prompt2(target_sentence, example):\n",
        "    return f\"\"\"\n",
        "    Translate the following Nahuatl sentence to Spanish:\n",
        "    Nahuatl: \"{example['nahuatl'].values[0]}\"\n",
        "    Spanish: \"{example['spanish'].values[0]}\"\n",
        "\n",
        "    Nahuatl: \"{target_sentence}\"\n",
        "    Spanish:\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "2dagqJPTWwkN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = create_one_shot_prompt2(nth_row['nahuatl'].values[0], one_shot_example)"
      ],
      "metadata": {
        "id": "C4bbxESNXFQr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando Gemini"
      ],
      "metadata": {
        "id": "Ln-gqNNEsVPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar la traducción con Gemini\n",
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "aJyXlyyfeTuK"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(prompt)"
      ],
      "metadata": {
        "id": "NTq5E5S_Xi2a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desplegar tanto la traducción generada por el modelo como la traducción esperada (de la fila seleccionada) para compararlas."
      ],
      "metadata": {
        "id": "-2OC-O3eiiQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la traducción generada\n",
        "print(\"Predicted:\", response.text)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6mNnIuXeUg2",
        "outputId": "d39b0cdc-0188-49d4-e2c5-0948584e2388"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Seamos reverentes con nuestros hijos y con los que reciben la enseñanza de Dios y en los libros de la escuela: si castigan mucho a los niños, Dios los castiga con plagas y con las enseñanzas escritas en libros.\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la función normalize se toma el texto y lo convierte a minúsculas y elimina cualquier puntuación o caracteres no alfanuméricos, para ayudar a hacer las comparaciones de las métricas de manera más justa."
      ],
      "metadata": {
        "id": "oPn3WqsHimwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para normalizar texto\n",
        "def normalize(text):\n",
        "    return re.sub(r'[^\\w\\s]', '', text.lower())  # Quita puntuación y convierte a minúsculas"
      ],
      "metadata": {
        "id": "AtFlWSRkeWTV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar referencia y predicción\n",
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(response.text)"
      ],
      "metadata": {
        "id": "Y_Cz5mjFeYRs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, estamos generando traducciones con un modelo preentrenado. Como este modelo no se entrena directamente, la preocupación de subajuste o sobreajuste se reduce, pero aún es posible evaluar la calidad de la traducción comparando los resultados obtenidos con la referencia.\n",
        "\n",
        "Evaluación del desempeño: Dado que se está utilizando un modelo generativo, la comparación con las métricas como METEOR, BLEU, y ROUGE nos ayuda a detectar si el modelo está generando traducciones de baja calidad.\n",
        "\n",
        "* METEOR score: Mide la calidad de las traducciones basándose en la correspondencia de n-gramas y sinónimos. El texto se tokeniza antes de calcular el score.\n",
        "* BLEU score: Mide la calidad de la traducción usando n-gramas de diferentes tamaños. En este caso se utiliza el suavizado para ayudar a evitar la penalización excesiva por falta de n-gramas en las traducciones.\n",
        "* ROUGE: Evalúa la calidad de la traducción en términos de precisión y recall a nivel de n-gramas y subsecuencias comúnes. 'Stemmer' ayuda a que las palabras se comparen en su forma raíz."
      ],
      "metadata": {
        "id": "hNQlqg-IobsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCqeDymseZyh",
        "outputId": "4c72a8c1-8889-4679-8c15-77de90f3da0a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.2567403108032203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKXGzVUJebBP",
        "outputId": "454a92c4-4d3c-48db-b3e4-10a56994246c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.09239364353597955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "totO0naBec52",
        "outputId": "0cdd3f36-d8ab-414b-dc1e-182deffd89f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': Score(precision=0.30952380952380953, recall=0.3611111111111111, fmeasure=0.3333333333333333), 'rouge2': Score(precision=0.04878048780487805, recall=0.05714285714285714, fmeasure=0.05263157894736842), 'rougeL': Score(precision=0.2619047619047619, recall=0.3055555555555556, fmeasure=0.2820512820512821)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, se puede observar que la traducción generada no coincide de forma esperada con la traducción esperada, lo cual es reflejado por las tres métricas de evaluación.\n",
        "\n",
        "* **METEOR score: 0.21**\n",
        " * Este es un valor bajo, indicando que hay poca coincidencia en la correspondencia de n-gramas entre la traducción generada y la referencia. Por lo que, las traducciones podrían ser significativamente diferentes.\n",
        "* **BLEU score: 0.12**\n",
        " * Tiene un valor bajo, lo que indica que la traducción generada no presenta una alta cantidad de n-gramas coincidentes con la traducción esperada.\n",
        "* **ROUGE scores:**\n",
        " * **rouge1: Precision 0.3, Recall 0.33, F-measure 0.32**\n",
        " Este score mide la coincidencia de unigrams, teniendo un rendimiento moderado. Aunque se tienen algunas palabras coincidentes, la traducción generada no captura el significado de la referencia.\n",
        " * **rouge2: Precision 0.0, Recall 0.0, F-measure 0.0**\n",
        " No hay coincidencias de bigramas entre la referencia y lo obtenido, lo cual no es un buen indicador en la calidad de la traducción.\n",
        " * **rougeL: Precision 0.21, Recall 0.22, F-measure 0.21**\n",
        " Este score evalúa subsecuencias más largas, el valor obtenido muestra que hay un poco de correspondencia en las secuencias de palabras, pero no el esperado.\n"
      ],
      "metadata": {
        "id": "ycajOd8JkLec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(prompt2)"
      ],
      "metadata": {
        "id": "eEfdjm_RXqmI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la traducción generada\n",
        "print(\"Predicted:\", response.text)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs-x47dfX1zq",
        "outputId": "a17a2a1b-e789-434b-e1e0-2c8b92211690"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Los hará saber a sus hijos y a los que vendrán después, aprendiendo del Libro de Dios y encontrando escrito lo siguiente: Si se rebela el pueblo, Dios desatará las plagas y entonces será escrito en el Libro. \"\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar referencia y predicción\n",
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(response.text)"
      ],
      "metadata": {
        "id": "G3MsH2QtX6G6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSydsxl_X8nZ",
        "outputId": "389178fb-428d-44e0-d403-d242c8923c8c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.22633350374914793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIQy23nRX_Lb",
        "outputId": "9ae6be4d-dbda-4ce8-cfc0-e9f9311968d4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0.09238425536915801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUbpyjs6YCGL",
        "outputId": "57b10937-8513-418d-b3d4-e20d2251ca34"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': Score(precision=0.325, recall=0.3611111111111111, fmeasure=0.34210526315789475), 'rouge2': Score(precision=0.02564102564102564, recall=0.02857142857142857, fmeasure=0.027027027027027025), 'rougeL': Score(precision=0.225, recall=0.25, fmeasure=0.2368421052631579)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, se puede observar que la traducción generada aún no coincide con lo esperado pero mejoro un poco comparado con el formato de prompt anterior.\n",
        "\n",
        "* **METEOR score: 0.23**\n",
        " * Este es un valor bajo, indicando que hay poca coincidencia en la correspondencia de n-gramas entre la traducción generada y la referencia. Por lo que, las traducciones podrían ser significativamente diferentes.\n",
        "* **BLEU score: 0.09**\n",
        " * Tiene un valor bajo, lo que indica que la traducción generada no presenta una alta cantidad de n-gramas coincidentes con la traducción esperada.\n",
        "* **ROUGE scores:**\n",
        " * **rouge1: Precision 0.33, Recall 0.36, F-measure 0.34**\n",
        " Este score mide la coincidencia de unigrams, teniendo un rendimiento moderado. Aunque se tienen algunas palabras coincidentes, la traducción generada no captura el significado de la referencia.\n",
        " * **rouge2: Precision 0.03, Recall 0.03, F-measure 0.03**\n",
        " Hay muy pocas coincidencias de bigramas entre la referencia y lo obtenido, lo cual no es un buen indicador en la calidad de la traducción.\n",
        " * **rougeL: Precision 0.23, Recall 0.25, F-measure 0.24**\n",
        " Este score evalúa subsecuencias más largas, el valor obtenido muestra que hay un poco de correspondencia en las secuencias de palabras, pero no el esperado."
      ],
      "metadata": {
        "id": "6hTMlnioY3rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_with_examples(target_sentence, examples):\n",
        "  prompt = \"Translate the following Nahuatl sentences to Spanish:\\n\\n\"\n",
        "  for example in examples:\n",
        "    prompt += f\"*   \\\"{example['nahuatl'].values[0]}\\\" -> \\\"{example['spanish'].values[0]}\\\"\\n\"\n",
        "\n",
        "  prompt += \"\\nNow, translate this sentence:\\n\\n\"\n",
        "  prompt += f\"\\\"{target_sentence}\\\"\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "2kzgoIKcaVvj"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_with_examples2(target_sentence, examples):\n",
        "  prompt = \"Translate the following Nahuatl sentences to Spanish:\\n\\n\"\n",
        "  for example in examples:\n",
        "    prompt += f\"\\nNahuatl: \\\"{example['nahuatl'].values[0]}\\\"  \\nSpanish: \\\"{example['spanish'].values[0]}\\\"\\n\"\n",
        "  prompt += f\"\\nNahuatl: \\\"{target_sentence}\\\"\"\n",
        "  prompt += \"\\nSpanish:\"\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "9DzH5eG2appn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = [20010, 20015, 20022]\n",
        "shot1 = df.iloc[[n[0]]]\n",
        "shot2 = df.iloc[[n[1]]]\n",
        "shot3 = df.iloc[[n[2]]]"
      ],
      "metadata": {
        "id": "SsNMB9Vyb4yG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = create_prompt_with_examples(nth_row[\"nahuatl\"].values[0], [shot1, shot2, shot3])"
      ],
      "metadata": {
        "id": "y2iVvH_acBDb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = create_prompt_with_examples2(nth_row[\"nahuatl\"].values[0], [shot1, shot2, shot3])"
      ],
      "metadata": {
        "id": "IYmxQDMDcChx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(prompt)"
      ],
      "metadata": {
        "id": "wsgjUEkYiovg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", response.text)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLZ1UvjakLCF",
        "outputId": "7ff3723f-84ff-4ae4-e9bc-9aefd72b0df0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted:  \"Amenazo a aquellos que alteran mis palabras y las enseñanzas del Dios viviente registradas en este libro: si alguien suprime algo de estas enseñanzas, Dios le quitará su parte en el libro de la vida y del árbol de la vida \"\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(response.text)\n",
        "\n",
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")\n",
        "\n",
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")\n",
        "\n",
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFwvk9RekO11",
        "outputId": "db5565c4-9c5e-40bb-bbf6-a2328939501b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.33923707154952093\n",
            "BLEU score: 0.2208630521496931\n",
            "ROUGE scores: {'rouge1': Score(precision=0.37209302325581395, recall=0.4444444444444444, fmeasure=0.40506329113924044), 'rouge2': Score(precision=0.11904761904761904, recall=0.14285714285714285, fmeasure=0.12987012987012989), 'rougeL': Score(precision=0.2558139534883721, recall=0.3055555555555556, fmeasure=0.27848101265822783)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, se puede observar que la traducción generada aún no coincide con lo esperado pero mejoro un poco comparado con el formato de prompt anterior.\n",
        "\n",
        "* **METEOR score: 0.34**\n",
        "* **BLEU score: 0.22**\n",
        "* **ROUGE scores:**\n",
        " * **rouge1: Precision 0.37, Recall 0.44, F-measure 0.40**\n",
        " * **rouge2: Precision 0.12, Recall 0.14, F-measure 0.13**\n",
        " * **rougeL: Precision 0.26, Recall 0.31, F-measure 0.28**."
      ],
      "metadata": {
        "id": "q3dNJJWUk5Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(prompt2)"
      ],
      "metadata": {
        "id": "rvjBJ5nglTYM"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", response.text)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVvGnRLVlg9q",
        "outputId": "07ff8b2d-aa3e-486d-f017-2fd536286a3d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Yo os felicito a los que me oís y guardáis estas palabras de la profecía. Porque el que las guarda, Dios le guardará de las plagas que están escritas en este libro.\"\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(response.text)\n",
        "\n",
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")\n",
        "\n",
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")\n",
        "\n",
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4kEZbmUlkPe",
        "outputId": "82b45c2b-3626-4958-d1e2-611610f3993c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.6304209183673469\n",
            "BLEU score: 0.5040161287741853\n",
            "ROUGE scores: {'rouge1': Score(precision=0.6388888888888888, recall=0.6388888888888888, fmeasure=0.6388888888888888), 'rouge2': Score(precision=0.4, recall=0.4, fmeasure=0.4000000000000001), 'rougeL': Score(precision=0.5277777777777778, recall=0.5277777777777778, fmeasure=0.5277777777777778)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, se puede observar que la traducción generada esta mucho más cerca de lo esperado!\n",
        "\n",
        "* **METEOR score: 0.63**\n",
        " * Este es el valor esperado, indicando que hay gran coincidencia en la correspondencia de n-gramas entre la traducción generada y la referencia. Por lo que, las traducciones podrían son similares.\n",
        "* **BLEU score: 0.50**\n",
        " * Se acerca más al valor esperado pero no completamente, lo que indica que la traducción generada presenta una alta cantidad de n-gramas coincidentes con la traducción esperada.\n",
        "* **ROUGE scores:**\n",
        " * **rouge1: Precision 0.64, Recall 0.64, F-measure 0.64**\n",
        " Este score mide la coincidencia de unigrams, teniendo un rendimiento moderado. Se tienen palabras que coinciden, la traducción generada captura la mayor parte del significado de la referencia.\n",
        " * **rouge2: Precision 0.4, Recall 0.4, F-measure 0.4**\n",
        " Hay coincidencias de bigramas entre la referencia y lo obtenido, lo cual es un buen indicador en la calidad de la traducción.\n",
        " * **rougeL: Precision 0.53, Recall 0.53, F-measure 0.53**\n",
        " Este score evalúa subsecuencias más largas, el valor obtenido se acerca ma´s al esperado y muestra que hay correspondencia en las secuencias de palabras."
      ],
      "metadata": {
        "id": "DsGdkFhzmcfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prueba usando T5 de HuggingFace"
      ],
      "metadata": {
        "id": "JWeuJJmWUepQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade protobuf\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P70swh1zZFnc",
        "outputId": "3c09c8ec-f295-4177-a338-3d7adb247639"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (4.25.6)\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "Successfully installed protobuf-5.29.3\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")"
      ],
      "metadata": {
        "id": "mFczDhAeUbW2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(prompt):\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "spanish_translation = translate(prompt)"
      ],
      "metadata": {
        "id": "JWHCH91bVCT0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar la traducción generada\n",
        "print(\"Predicted:\", spanish_translation)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_SIIRdoVlu1",
        "outputId": "47c937f5-19a5-42c8-eaa7-8403c69d4cf3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: yejhuan ye tlajcuilolamat\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(spanish_translation)"
      ],
      "metadata": {
        "id": "SQm1f2qzVq98"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRmxDDS7V_1f",
        "outputId": "66578228-7aa4-41c4-924e-aa58904e0545"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaZeVMHaWCK_",
        "outputId": "11d20b33-8842-4fad-a4ca-f584d95bbdf0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQDvpfCMWEHY",
        "outputId": "12cabe30-db6c-4f72-c64d-60d4a2b35f30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso, se puede observar que la traducción generada no coincide de forma esperada con la traducción esperada, lo cual es reflejado por las tres métricas de evaluación.\n",
        "\n",
        "* **METEOR score: 0.0**\n",
        " * Este es un valor bajo, indicando que hay poca coincidencia en la correspondencia de n-gramas entre la traducción generada y la referencia. Por lo que, las traducciones podrían ser significativamente diferentes.\n",
        "* **BLEU score: 0**\n",
        " * Tiene un valor bajo, lo que indica que la traducción generada no presenta una alta cantidad de n-gramas coincidentes con la traducción esperada.\n",
        "* **ROUGE scores:**\n",
        " * **rouge1: Precision 0.0, Recall 0.0, F-measure 0.0**\n",
        " Este score mide la coincidencia de unigrams, teniendo un rendimiento moderado. No hay coincidencia.\n",
        " * **rouge2: Precision 0.0, Recall 0.0, F-measure 0.0**\n",
        " No hay coincidencias de bigramas entre la referencia y lo obtenido, lo cual no es un buen indicador en la calidad de la traducción.\n",
        " * **rougeL: Precision 0.0, Recall 0.0, F-measure 0.0**\n",
        " Este score evalúa subsecuencias más largas, el valor obtenido muestra que no hay correspondencia en las secuencias de palabras"
      ],
      "metadata": {
        "id": "16F8ZiHYaMgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_translation = translate(prompt2)"
      ],
      "metadata": {
        "id": "8vGuXo6xZ5l7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", spanish_translation)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKA9xaVmZ8AA",
        "outputId": "126f6dcf-4677-4d50-d386-b84a6feeae1a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: yejhuan ye tlajcuilolamat\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(spanish_translation)"
      ],
      "metadata": {
        "id": "u01rMdvhZ__c"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQLNr1QqaCOv",
        "outputId": "1a1e73d3-58c9-4e28-fa93-f271d9d6666e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjW0CSjIaEOF",
        "outputId": "037b04ed-5b14-46e3-b8ae-4e1e32de11a9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEwEYeX5aGeu",
        "outputId": "098630a1-a277-4fa3-f2a4-3e613beae053"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_translation = translate(prompt)"
      ],
      "metadata": {
        "id": "isSX3UatoKKQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", spanish_translation)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iADQmydsoPiW",
        "outputId": "f25583d4-06e1-44b7-ccda-02e203592861"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: tlajtlamach yejhuan yejh\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(spanish_translation)\n",
        "\n",
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")\n",
        "\n",
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")\n",
        "\n",
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q17HjzL5oTUg",
        "outputId": "d8790820-ffef-4064-9f16-51a14c645d39"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.0\n",
            "BLEU score: 0\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spanish_translation = translate(prompt2)"
      ],
      "metadata": {
        "id": "eyvVAph1oeEA"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted:\", spanish_translation)\n",
        "print(\"Expected:\", nth_row['spanish'].values[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x3L-sPaoh2H",
        "outputId": "1c1be89f-f610-440b-ccc8-03bf01bfafe1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: tlajtlamach yejhuan yejh\n",
            "Expected:  Yo advierto a todo el que oye las palabras de la profecía de este libro: Si alguno añade a estas cosas, Dios le añadirá las plagas que están escritas en este libro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference = normalize(nth_row['spanish'].values[0])\n",
        "hypothesis = normalize(spanish_translation)\n",
        "\n",
        "# Calcular METEOR score\n",
        "meteor = meteor_score([word_tokenize(reference)], word_tokenize(hypothesis))\n",
        "print(f\"METEOR score: {meteor}\")\n",
        "\n",
        "# Calcular BLEU score con smoothing y BLEU-2\n",
        "smoother = SmoothingFunction().method4\n",
        "bleu = sentence_bleu([word_tokenize(reference)], word_tokenize(hypothesis), weights=(0.5, 0.5), smoothing_function=smoother)\n",
        "print(f\"BLEU score: {bleu}\")\n",
        "\n",
        "# Calcular ROUGE score\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = scorer.score(reference, hypothesis)\n",
        "print(f\"ROUGE scores: {rouge_scores}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ugz6k4iojTa",
        "outputId": "5c256b00-808e-42ac-b2f1-5c7fb48ee142"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.0\n",
            "BLEU score: 0\n",
            "ROUGE scores: {'rouge1': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.0, recall=0.0, fmeasure=0.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion y siguientes pasos"
      ],
      "metadata": {
        "id": "7Gyr5lOPuIWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entre estos 2 modelos escogemos a Gemini debido a que produjo mejores resultados. Tambien vimos que al agregar más shots los scores mejoraron y se acercan a lo esperado. Cambiar el formato del prompt también cambio el score.\n",
        "\n",
        "En el futuro podemos intentar con más shots y distintos formatos del prompt para ver como mejora.\n",
        "\n",
        "En un futuro podemos volver a comparar con otros modelos de costo como openAI si tenemos los recursos para ello."
      ],
      "metadata": {
        "id": "VRrSe5beuL1Q"
      }
    }
  ]
}